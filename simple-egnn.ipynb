{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a5090",
   "metadata": {},
   "source": [
    "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
    "\n",
    "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bU4ixrOJCg1",
   "metadata": {
    "id": "4bU4ixrOJCg1"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb08a10",
   "metadata": {},
   "source": [
    "# Load QM9 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae30de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'simple-equivariant-gnn'...\n",
      "remote: Enumerating objects: 84, done.\u001b[K\n",
      "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
      "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
      "remote: Total 84 (delta 35), reused 31 (delta 5), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (84/84), 175.01 KiB | 1.58 MiB/s, done.\n",
      "Resolving deltas: 100% (35/35), done.\n",
      "/Users/ars/Downloads/simple-equivariant-gnn-main/simple-equivariant-gnn\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/senya-ashukha/simple-equivariant-gnn.git\n",
    "%cd simple-equivariant-gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859f981c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "859f981c",
    "outputId": "3b62e11b-79be-4cbd-f9ff-38ccc05d013b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/qm9/data/prepare/process.py:126: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if line[0] is '#':\n",
      "/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/qm9/data/prepare/process.py:128: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if line_counter is 0:\n",
      "/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/qm9/data/prepare/process.py:130: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif line_counter is 1:\n",
      "/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/qm9/data/prepare/process.py:143: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if len(split) is 4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 6, 7, 8, 9])\n",
      "dict_keys([0, 1, 6, 7, 8, 9])\n",
      "dict_keys([0, 1, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
    "# We will predict Highest occupied molecular orbital energy \n",
    "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
    "# We use data loaders from the official repo\n",
    "\n",
    "from qm9.data_utils import get_data, BatchGraph\n",
    "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59a8059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_atoms', 'charges', 'positions', 'index', 'A', 'B', 'C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'U0', 'U', 'H', 'G', 'Cv', 'omega1', 'zpve_thermo', 'U0_thermo', 'U_thermo', 'H_thermo', 'G_thermo', 'Cv_thermo', 'one_hot', 'atom_mask', 'edge_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e20004",
   "metadata": {},
   "source": [
    "# Graph Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0acbcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In the batch: num_graphs 96 num_nodes 1759\n",
       "> .h \t\t a tensor of nodes representations \t\tshape 1759 x 15\n",
       "> .x \t\t a tensor of nodes positions  \t\t\tshape 1759 x 3\n",
       "> .edges \t a tensor of edges, a fully connected graph \tshape 31288 x 2\n",
       "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c0726",
   "metadata": {},
   "source": [
    "# Define Equivariant Graph Convs  & GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e5e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_sum(agg_size, source, idx, cuda):\n",
    "    \"\"\"\n",
    "        source is N x hid_dim [float]\n",
    "        idx    is N           [int]\n",
    "        \n",
    "        Sums the rows source[.] with the same idx[.];\n",
    "    \"\"\"\n",
    "    print(agg_size, source, idx)\n",
    "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
    "    tmp = tmp.cuda() if cuda else tmp\n",
    "    res = torch.index_add(tmp, 0, idx, source)\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "    # (self, dim, index, source)\n",
    "    # Tensor.index_add_(dim, index, source, *, alpha=1)\n",
    "    # self[index[i], :, :] += 1 * src[i, :, :]  # if dim == 0\n",
    "    # essentially aggregating along idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5d55db",
   "metadata": {
    "id": "4d5d55db"
   },
   "outputs": [],
   "source": [
    "class ConvEGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        self.cuda = cuda\n",
    "        \n",
    "        # computes messages based on hidden representations -> [0, 1]\n",
    "        self.f_e = nn.Sequential(\n",
    "            nn.Linear(in_dim*2+1, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU())\n",
    "        \n",
    "        # predicts \"soft\" edges based on messages \n",
    "        self.f_inf = nn.Sequential( \n",
    "            nn.Linear(hid_dim, 1),\n",
    "            nn.Sigmoid()) \n",
    "        \n",
    "        # updates hidden representations -> [0, 1]\n",
    "        self.f_h = nn.Sequential(\n",
    "            nn.Linear(in_dim+hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim))\n",
    "    \n",
    "    def forward(self, b):\n",
    "        e_st, e_end = b.edges[:,0], b.edges[:,1]\n",
    "        dists = torch.norm(b.x[e_st] - b.x[e_end], dim=1).reshape(-1, 1) # Euclidean distance between the start and end edge\n",
    "        \n",
    "        # compute messages\n",
    "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
    "        m_ij = self.f_e(tmp) # message at position ij\n",
    "        \n",
    "        # predict edges\n",
    "        e_ij = self.f_inf(m_ij) # edge prediction at position ij\n",
    "        \n",
    "        # average e_ij-weighted messages  \n",
    "        # m_i is num_nodes x hid_dim\n",
    "        m_i = index_sum(b.h.shape[0], e_ij*m_ij, b.edges[:,0], self.cuda) # average message at position ij\n",
    "        \n",
    "        # update hidden representations\n",
    "        b.h += self.f_h(torch.hstack([b.h, m_i])) # backprop\n",
    "\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10aad7c4",
   "metadata": {
    "id": "10aad7c4"
   },
   "outputs": [],
   "source": [
    "class NetEGNN(nn.Module):\n",
    "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        \n",
    "        self.emb = nn.Linear(in_dim, hid_dim) \n",
    "\n",
    "        self.gnn = [ConvEGNN(hid_dim, hid_dim, cuda=cuda) for _ in range(n_layers)]\n",
    "        self.gnn = nn.Sequential(*self.gnn)\n",
    "        \n",
    "        # encoder\n",
    "        self.pre_mlp = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim))\n",
    "        \n",
    "        # decoder\n",
    "        self.post_mlp = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, out_dim))\n",
    "\n",
    "        if cuda: self.cuda()\n",
    "        self.cuda = cuda\n",
    "    \n",
    "    def forward(self, b):\n",
    "        b.h = self.emb(b.h)\n",
    "        \n",
    "        b = self.gnn(b)\n",
    "        h_nodes = self.pre_mlp(b.h)\n",
    "        \n",
    "        # h_graph is num_graphs x hid_dim\n",
    "        # h_nodes provides an embedding of all graphs \n",
    "        # index_sum provides an aggregate of all graphs \n",
    "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
    "        \n",
    "        out = self.post_mlp(h_graph)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f4cef6",
   "metadata": {
    "id": "b7f4cef6"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cuda \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m NetEGNN(n_layers\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m, cuda\u001b[39m=\u001b[39;49mcuda)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-16\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m lr_scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[39m=\u001b[39mepochs, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb Cell 13\u001b[0m in \u001b[0;36mNetEGNN.__init__\u001b[0;34m(self, in_dim, hid_dim, out_dim, n_layers, cuda)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_mlp \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(hid_dim, hid_dim), nn\u001b[39m.\u001b[39mSiLU(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(hid_dim, hid_dim))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_mlp \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.4\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(hid_dim, hid_dim), nn\u001b[39m.\u001b[39mSiLU(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(hid_dim, out_dim))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m cuda: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/CS260FinalProject/simple-equivariant-gnn/simple-egnn.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcuda \u001b[39m=\u001b[39m cuda\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:747\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    731\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:747\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    731\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/cuda/__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "cuda = True\n",
    "\n",
    "model = NetEGNN(n_layers=7, cuda=cuda)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d6b1c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3613c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de3613c9",
    "outputId": "a924add2-aadc-4669-e7cb-3a92c13ba5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> start training\n",
      "> epoch 000: "
     ]
    }
   ],
   "source": [
    "print('> start training')\n",
    "\n",
    "tr_ys = train_loader.dataset.data['homo'] \n",
    "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
    "\n",
    "if cuda:\n",
    "    me = me.cuda()\n",
    "    mad = mad.cuda()\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
    "    start = time.time()\n",
    "\n",
    "    batch_train_loss = []\n",
    "    batch_val_loss = []\n",
    "    batch_test_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = BatchGraph(batch, cuda, charge_scale)\n",
    "        \n",
    "        out = model(batch).reshape(-1)\n",
    "        loss =  F.l1_loss(out,  (batch.y-me)/mad)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
    "\n",
    "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
    "        \n",
    "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
    "    \n",
    "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_val_loss += [np.mean(loss)]\n",
    "            \n",
    "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
    "        \n",
    "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_test_loss += [np.mean(loss)]\n",
    "\n",
    "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > start training \n",
    "# > epoch 000: train 264.008 val 243.351 test 242.635 (94.3 sec)\n",
    "# > epoch 001: train 211.893 val 211.575 test 210.144 (92.8 sec)\n",
    "# > epoch 002: train 185.362 val 164.960 test 165.087 (93.9 sec)\n",
    "# > epoch 003: train 163.121 val 150.953 test 150.533 (93.2 sec)\n",
    "# ...\n",
    "# > epoch 998: train 0.032 val 30.157 test 30.886 (93.4 sec)\n",
    "# > epoch 999: train 0.032 val 30.157 test 30.886 (93.4 sec)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "graph_seminar_homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
